---
title: "CS231_n Image Classification - part1"
date: 2020-05-05T12:00:00+08:00
description: "LeetCode Solving"
draft: false
hideToc: false
enableToc: true
enableTocContent: false
author: Stanley
authorEmoji: ğŸ«
tags:
- cs231n
- Image Classification
- KNN
- python
- Deep learning
series:
- cs231_n
categories:
- cs231_n
- CNN
- DeepLearning
image: images/feature2/mathbook.png
---

### å‰è¨€

å½±åƒåˆ†é¡(Image Classification)ä¸»è¦çš„ä½œæ³•æ˜¯å°‡è¼¸å…¥çš„å½±åƒåšè¾¨è­˜ä¸¦ä¸”åˆ†é–€åˆ¥é¡ã€‚é€™å°æ–¼é›»è…¦è¦–è¦ºçš„é ˜åŸŸä¾†èªªï¼Œæ˜¯é‡è¦ä¸”æ ¸å¿ƒçš„è­°é¡Œä¹‹ä¸€ã€‚æ¯”å¦‚èªªæˆ‘å€‘æƒ³åœ¨ä¸€å¼µåœ–ç‰‡ä¸Šåˆ†è¾¨äººã€è²“ã€è»Šå­ï¼Œæˆ–æ˜¯åœ¨ä¸€å †è»Šå­çš„å½±åƒä¸­åˆ†è¾¨å‡ºä¸åŒå“ç‰Œæˆ–æ˜¯ä¸åŒç¨®é¡çš„è»Šå­ã€‚

**é‚£æ€éº¼å»åšå½±åƒåˆ†é¡å‘¢? é¦–å…ˆæˆ‘å€‘è¦å…ˆäº†è§£å½±åƒçš„çµæ§‹ã€‚**

æˆ‘å€‘çŸ¥é“å…‰çš„ä¸‰åŸè‰²æ˜¯ _R G B_ ï¼Œåœ¨é›»è…¦ç¡¬é«”ä¸Šæ¡å–æ¯ä¸€å€‹åƒç´ ä»¥24ä½å…ƒçš„è¡¨ç¤ºæ–¹å¼ï¼Œæ¯å€‹ä½å…ƒä»¥äºŒé€²ä½æ–¹å¼å„²å­˜ï¼Œé‚£å…‰çš„ä¸‰åŸè‰²å°±å„ä½”äº†1/3ï¼Œå°±æ˜¯8bitsã€‚æ‰€ä»¥æ¯ä¸€ç¨®åŸè‰²çš„å¼·åº¦å°±æœƒä¾ç…§8bitsçš„æœ€é«˜å€¼2<sup>8</sup>åˆ†æˆ256å€‹å€¼ï¼Œè¡¨ç¤ºç¯„åœå¾0~255ã€‚

å‡è¨­ä¸€å¼µ 800 x 600 çš„å½©è‰²åœ–ç‰‡ï¼Œè£¡é¢æ‰€åŒ…å«çš„è³‡è¨Šå°±æœƒæœ‰  
> 800 x 600 x 3 = 1440000

å‡è¨­è§£æåº¦æ›´å¥½ï¼Œæ¯”å¦‚èªªæ˜¯4Kè§£æåº¦ï¼Œé‚£è³‡è¨Šé‡åŸºæœ¬ä¸Šæ˜¯éå¸¸é¾å¤§çš„ã€‚
<br/>
![img1](/images/CS231_n/img_classification/img1.png)
<br/><br/>

### Challenges

æ ¹æ“šæ‹æ”çš„å ´æ™¯èˆ‡æƒ…å½¢ï¼Œå…¶å¯¦æœƒé€ æˆå¾ˆå¤šåœ¨å¯¦éš›ä¸Šæ‡‰ç”¨çš„é›£è™•ã€‚æˆ‘å€‘é€™é‚Šè¤‡è£½cs231nçš„ç­†è¨˜å…§å®¹ä¾†ç°¡å–®è¬›è¿°ä¸€ä¸‹æœƒé‡åˆ°çš„å•é¡Œã€‚
<br/><br/>
![challenges](/images/CS231_n/img_classification/challenges.jpeg)
<br/><br/>
* **Viewpoint variation.** A single instance of an object can be oriented in many ways with respect to the camera.

* **Scale variation.** Visual classes often exhibit variation in their size (size in the real world, not only in terms of their extent in the image).

* **Deformation.** Many objects of interest are not rigid bodies and can be deformed in extreme ways.

* **Occlusion.** The objects of interest can be occluded. Sometimes only a small portion of an object (as little as few pixels) could be visible.

* **Illumination conditions.** The effects of illumination are drastic on the pixel level.

* **Background clutter.** The objects of interest may blend into their environment, making them hard to identify.

* **Intra-class variation.** The classes of interest can often be relatively broad, such as chair. There are many different types of these objects, each with their own appearance.
<br/><br/>

#### Viewpoint variation
é‡å°åŒä¸€å€‹ç‰©é«”ä¾†èªªï¼Œå‡è¨­æˆ‘å€‘ä»Šå¤©å¾ä¸åŒè§’åº¦æ‹æ”ä»–ï¼Œå¯èƒ½å› ç‚ºå‘¨é­çš„å…‰ç·šæˆ–æ˜¯æœ‰é®è”½ç‰©ç­‰ç­‰çš„å› ç´ ï¼Œæœƒé€ æˆpixelçš„è®ŠåŒ–ã€‚pixelä¸€ä½†æœ‰è®ŠåŒ–ï¼Œæœ‰å¯èƒ½æœƒé€ æˆè¾¨åˆ¥çš„å›°é›£å¢åŠ ã€‚
<br/><br/>

![img2_viewpoint](/images/CS231_n/img_classification/img2_viewpoint.PNG)
<br/><br/>

#### Scale variation

è¾¨è­˜çš„ç‰©ä»¶ç›¸å°æ–¼å…¶ä»–ç‰©ä»¶çš„å°æ¯”å¤§å°ï¼ŒåŒ…å«åœ¨å½±åƒè£¡èˆ‡ç¾å¯¦ç”Ÿæ´»ä¸­ã€‚
<br/><br/>

#### Illumination conditions
å‰›å‰›æœ‰æåˆ°é›»è…¦ç¡¬é«”ä¸­çš„ä¸‰åŸè‰²å„²å­˜æ–¹å¼ï¼Œ0~255è¡¨ç¤ºè©²åŸè‰²çš„äº®åº¦ã€‚ä½ æ‡‰è©²çœ‹éæœ‰äº›ç…§ç‰‡åœ¨é€†å…‰ä¸‹ï¼Œäººçš„è‡‰å­”æ ¹æœ¬å®Œå…¨çœ‹ä¸æ¸…æ¥šã€‚æ‰€ä»¥ç’°å¢ƒçš„æ˜æš—ç¨‹åº¦ï¼Œå°‡æœƒå½±éŸ¿åˆ°è¾¨è­˜çš„æ­£ç¢ºç‡ã€‚
<br/><br/>

![img2_illumination](/images/CS231_n/img_classification/img2_Illumination.PNG)

<br/><br/>

#### Deformation
æœ‰çœ‹åˆ°æˆ‘å€‘å¯æ„›çš„è²“è²“å—? ä¿—è©±èªªè²“è²“å°±è·Ÿæ¶²é«”ä¸€æ¨£æœƒè®Šå½¢ï¼Œè®Šå½¢ä¹‹å¾Œæ€éº¼æœ‰è¾¦æ³•è®“é›»è…¦è¾¨è­˜å¾—å‡ºä¾†å‘¢? ç¾å¯¦ç”Ÿæ´»ä¸­ç”Ÿç‰©éƒ½æ˜¯éå‰›é«”çš„ï¼Œæ‰€ä»¥è®Šå½¢çš„æƒ…æ³ä¹Ÿæ˜¯éœ€è¦å»å…‹æœçš„ã€‚
<br/><br/>

![img2_deformation](/images/CS231_n/img_classification/img2_deformation.PNG)

<br/>

> é€™è£¡çš„ Deformation æŒ‡çš„æ˜¯é«”æ…‹çš„ä¸åŒï¼Œä¸¦ä¸æ˜¯åƒå‰é¢viewpointä¸­ï¼Œåœ¨ä¸åŒè§’åº¦çœ‹åŒä¸€å€‹ç‰©ä»¶çš„å½¢ç‹€ä¸åŒã€‚

<br/><br/>

#### Occlusion
ä¸­æ–‡ç¿»è­¯å«åšå’¬åˆï¼Œå¯¦éš›ä¸Šçš„æ„æ€å°±æ˜¯èªªï¼Œæœ‰èˆˆè¶£çš„ç‰©ä»¶è¢«éƒ¨åˆ†é®è”½äº†ã€‚æˆ‘å€‘å¯èƒ½çœ‹åˆ°è²“å°¾å·´ï¼Œå°±çŸ¥é“é‚£é‚Šæœ‰ä¸€éš»è²“ï¼Œä½†æ˜¯é›»è…¦å»åˆ†é¡ä¸å‡ºä¾†ã€‚
<br/><br/>

![img2_occlusion](/images/CS231_n/img_classification/img2_occlusion.PNG)

> ç‰©ä»¶è¢«éƒ¨åˆ†é®è”½äº†ï¼Œé¡¯éœ²å‡ºä¾†çš„åœ°æ–¹åªæœ‰å¹¾å€‹åƒç´ çš„å¤§å°ã€‚

<br/><br/>

#### Background clutter
ç™½è²“åœ¨é›ªåœ°ï¼Œé»‘è²“ç«™åœ¨é»‘å¸ƒä¸Šï¼Œæ©˜è²“åœ¨æ¯é»ƒçš„è‰åœ°ä¸Šäº«å—å¤•é™½ï¼Œå¦‚æœé€£æˆ‘å€‘éƒ½ä¸ä»”ç´°çœ‹ï¼Œå°±æœƒéºæ¼å•Š!æ›´ä½•æ³æ˜¯é›»è…¦äº†ï¼Œæ ¹æœ¬è¾¨è­˜ä¸å‡ºä¾†ã€‚
<br/><br/>

![img2_backgroundclutter](/images/CS231_n/img_classification/img2_backgroundclutter.PNG)
<br/><br/>
> ç°¡å–®ä¾†è¬›å°±æ˜¯ï¼Œç’°å¢ƒä¿è­·è‰²ã€‚

<br/><br/>

#### Intraclass variation
ç›¸åŒåç¨±ä½†æ˜¯å»ä¸åŒå½¢ç‹€ã€é¡è‰²ï¼Œåƒæ˜¯åœ–ç‰‡ä¸Šçš„è²“å’ªé¡è‰²ä¸åŒï¼Œæˆ–æ˜¯æ¤…å­æœ‰ä¸åŒçš„å½¢ç‹€ã€‚è©²æ€éº¼å»åˆ†é–€åˆ¥é¡ï¼Œå°±è¦é‡å°å¯¦éš›æ‡‰ç”¨å›‰!
<br/><br/>

![img2_intraclass](/images/CS231_n/img_classification/img2_intraclass.PNG)

<br/><br/>

### è³‡æ–™é©…å‹•çš„è™•ç†æ–¹å¼ (Data-driven Approach)

ä¸€èˆ¬æˆ‘å€‘åœ¨å¯«æ’åºç›¸é—œçš„æ¼”ç®—æ³•ï¼Œéƒ½æ˜¯åœ¨ç¨‹å¼è£¡é¢æŒ‡å®šèªªæˆ‘å€‘æƒ³ç”¨ä»€éº¼æ–¹å¼ä¾†åšã€‚ä½†å°æ–¼è™•ç†å½±åƒè¾¨è­˜çš„å•é¡Œï¼Œå› ç‚ºè¾¨è­˜çš„æƒ…æ³ä¾ç…§å„è‡ªé ˜åŸŸçš„ä¸åŒï¼Œæ¼”ç®—æ³•ä¸¦ä¸æ˜¯é‚£éº¼çµ±ä¸€ï¼Œå› æ­¤å¤§å¤šæ¡ç”¨è³‡æ–™é©…å‹•çš„è™•ç†æ–¹å¼ _(Data-driven Approach)_ ã€‚

æä¾›å¤§é‡ä¸åŒé¡åˆ¥çš„examplesï¼Œä¸¦ç™¼å±•ä¸€å€‹ã€Œå­¸ç¿’æ¼”ç®—æ³•ã€ä¾†ç€è¦½é€™äº›examplesï¼Œä¸¦å­¸ç¿’é€™äº›é¡åˆ¥çš„ **ã€Œè¦–è¦ºå¤–è§€ã€** ã€‚å­¸ç¿’çš„è³‡æ–™é›†æˆ‘å€‘ç¨±åš **traning set**ï¼Œæ¸¬è©¦å®Œä¹‹å¾Œè¦é©—è­‰æ­£ç¢ºæ€§çš„è³‡æ–™é›†ç¨±åš **testing set**ã€‚å…¶å¯¦é€™ç¨®å­¸ç¿’çš„æ–¹æ³•ï¼Œæˆ‘å€‘å°±ç¨±åšæ˜¯ _æ©Ÿå™¨å­¸ç¿’_ ã€‚

é‚£éº¼æ•´å€‹æ©Ÿå™¨å­¸ç¿’çš„æµç¨‹æ˜¯ä»€éº¼å‘¢?

> 1. æ”¶é›†å½±åƒ _(images)_ èˆ‡æ¨™ç±¤ _(label)_ çš„è³‡æ–™é›†
> 2. åˆ©ç”¨æ©Ÿå™¨å­¸ç¿’çš„æ–¹æ³•ä¾†è¨“ç·´åˆ†é¡å™¨ _(classifier)_
> 3. åœ¨æ–°çš„å½±åƒä¸Šè©•ä¼°é€™å€‹åˆ†é¡å™¨æ˜¯å¦æœ‰æ•ˆ

ä»¥ä¸Šçš„labelå°±æ˜¯é¡åˆ¥ä¸€æ¨£ï¼Œæ¯”å¦‚èªªæ¨™è¨˜æˆç‹—ã€è²“ã€é£›æ©Ÿä¹‹é¡çš„ã€‚

ä»¥CIFAR10ç‚ºä¾‹
<br/>

![img3_classifier](/images/CS231_n/img_classification/img3_classifier.png)
<br/><br/>
é€™é‚Šé™„ä¸Š CS231_n çš„ image classification pipelineåŸæ–‡

* **Input:** Our input consists of a set of N images, each labeled with one of K different classes. We refer to this data as the training set.

* **Learning:** Our task is to use the training set to learn what every one of the classes looks like. We refer to this step as training a classifier, or learning a model.

* **Evaluation:** In the end, we evaluate the quality of the classifier by asking it to predict labels for a new set of images that it has never seen before. We will then compare the true labels of these images to the ones predicted by the classifier. Intuitively, weâ€™re hoping that a lot of the predictions match up with the true answers (which we call the ground truth).

<br/><br/>

### Classification
#### Nearest Neighbor Classifier
å½±åƒè¾¨è­˜å¸¸æœƒç”¨åˆ° _Convolutional Neural Networks (CNN)_ ï¼Œä½†æ˜¯é€™å€‹Nearest Neighbor Classifierå»è·ŸCNNæ¯«ç„¡ç›¸é—œï¼Œå……å…¶é‡åªæ˜¯è®“æˆ‘å€‘äº†è§£æ©Ÿå™¨å­¸ç¿’çš„ä¸€ç¨®è™•ç†æ–¹å¼è€Œå·²ã€‚

CS231_n ç­†è¨˜è£¡é¢æåˆ° nearest neighbor classifierçš„è™•ç†æ–¹å¼:
> The nearest neighbor classifier will take a test image, compare it to every single one of the training images, and predict the label of the closest training image.

æœ‰å…©å€‹é‡é»

1. æ‰¾test imageè·Ÿ"æ¯ä¸€å€‹" train imageåšæ¯”è¼ƒã€‚
2. åˆ¤æ–·æ­¤test imageè·Ÿlabelçš„é—œä¿‚é è¿‘ã€‚

ç¬¬ä¸€é»å¾ˆå¥½ç†è§£ï¼Œå°±æ˜¯æŠŠæŸå¼µæ¸¬è©¦é›†çš„åœ–ç‰‡ï¼Œå°è¨“ç·´é›†çš„æ¯ä¸€å¼µåœ–ç‰‡é€²è¡Œæ¯”å°ï¼Œç„¶å¾Œç”¨åˆ†æ•¸ï¼Œä¹Ÿå°±æ˜¯ä¸€å€‹å‡½æ•¸ä¾†è©•ä¼°æ˜¯å¦å±¬æ–¼é€™å€‹é¡åˆ¥ã€‚

ç¬¬äºŒé»ï¼Œä¹Ÿå°±æ˜¯é—œæ–¼é€™å€‹å‡½æ•¸ï¼Œè©²ç”¨ä»€éº¼å‡½æ•¸ä¾†è¡¨ç¤ºä¹‹é–“é—œä¿‚çš„é è¿‘ç¨‹åº¦? çµ•å°å€¼å°±å¯ä»¥å—? å…©é»è·é›¢å…¬å¼å—? é€™å€‹å•é¡Œå°±æ˜¯ **Metric** æˆ–ç¨±ä½œæ˜¯ **distance function** ï¼Œè©³ç´°éç¨‹ä¸åœ¨é€™é‚Šæ•˜è¿°ï¼Œå¯ä»¥åƒè€ƒä»¥ä¸‹è³‡æ–™ã€‚

[**Metric**](https://en.wikipedia.org/wiki/Metric_(mathematics))   
[**metric-learn: Metric Learning in Python**](http://contrib.scikit-learn.org/metric-learn/index.html)
<br/><br/>
é€™é‚Šçš„distance functionæ˜¯æ¡ç”¨ä¸‹åˆ—çš„æ–¹å¼è¨ˆç®—çš„ã€‚æ¯ä¸€æ ¼çš„å€¼ç›¸æ¸›å®Œä¹‹å¾Œï¼Œå–å®Œçµ•å°å€¼åœ¨å°‡æ¯ä¸€æ ¼ç›¸åŠ èµ·ä¾†ã€‚
<br/>

![img4_metric](/images/CS231_n/img_classification/img4_metric.PNG)
<br/><br/>
ç°¡å–®çš„Nearest Neighbor Classifierå¯¦ä½œå¤§æ¦‚å¦‚ä¸‹ã€‚å¦‚æœè¦è©³ç´°çœ‹ä»‹ç´¹çš„è©±

[**nearest-neighbor-classifier**](https://cs231n.github.io/classification/#nearest-neighbor-classifier)

é€™é‚Šå°±ä¸å†èªªæ˜ **nearest-neighbor-classifier** è©²å¦‚ä½•å¯¦ä½œäº†ï¼Œå› ç‚ºæº–ç¢ºæ€§ä¹Ÿä¸é«˜ã€‚

```Python
import numpy as np

class NearestNeighbor(object):
  def __init__(self):
    pass

  def train(self, X, y):
    """ X is N x D where each row is an example. Y is 1-dimension of size N """
    # the nearest neighbor classifier simply remembers all the training data
    self.Xtr = X
    self.ytr = y

  def predict(self, X):
    """ X is N x D where each row is an example we wish to predict label for """
    num_test = X.shape[0]
    # lets make sure that the output type matches the input type
    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)

    # loop over all test rows
    for i in range(num_test):
      # find the nearest training image to the i'th test image
      # using the L1 distance (sum of absolute value differences)
      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)
      min_index = np.argmin(distances) # get the index with smallest distance
      Ypred[i] = self.ytr[min_index] # predict the label of the nearest example

    return Ypred
```

<br/><br/>

#### k - Nearest Neighbor Classifier (KNN)

[çœ‹æˆ‘å¦‚ä½•å¯¦ä½œknn (ongoing)]

> æ‰¾å‡ºkå€‹èˆ‡ç›®æ¨™æ¯”è¼ƒç›¸è¿‘çš„åœ–ç‰‡ï¼Œä¸¦è®“é€™kå€‹åœ–ç‰‡ã€ŒæŠ•ç¥¨ã€é¸å‡ºç›®æ¨™çš„é¡åˆ¥ã€‚

æ‰€ä»¥ä¸Šè¿°æ¢ä»¶åœ¨ _k = 1_ æ™‚ï¼Œå°±æ˜¯å‰ä¸€å€‹ç« ç¯€æåˆ°çš„ **Nearest Neighbor Classifier** ã€‚
<br/><br/>

![knn](/images/CS231_n/img_classification/knn.jpeg)
<br/><br/>
æ‰€ä»¥æˆ‘å€‘ç¾åœ¨çš„å•é¡Œæ˜¯ï¼Œ_k_ åˆ°åº•è©²æ€éº¼é¸æ“‡?

<br/><br/>

#### è¶…åƒæ•¸èª¿æ•´ (Hyperparameter tuning)

_k_ å¦‚ä½•é¸æ“‡? è©²é¸æ“‡ä»€éº¼æ¨£çš„distance function? é€™äº›é¸æ“‡æˆ‘å€‘å«ä½œè¶…åƒæ•¸ _(hyperparameter)_ ï¼Œä¹Ÿå°±æ˜¯å¦‚ä½•è¨­ç½®æ¼”ç®—æ³•çš„åƒæ•¸ã€‚é‚£è©²å¦‚ä½•é¸æ“‡é€™äº›åƒæ•¸å‘¢?

> ä¸€å€‹ä¸€å€‹èª¿æ•´ã€‚

æ²’éŒ¯å°±æ˜¯ä¸€å€‹ä¸€å€‹èª¿æ•´ã€‚

**k = 1, performance = ...**
**k = 2, performance = ...**
**k = 3, performance = ...**

ä¸¦ä¸”åƒæ•¸çš„èª¿æ•´é‚„æ˜¯ **problem-dependent** ï¼Œå–æ±ºæ–¼ä½ æƒ³è¦ç”¨åœ¨åœ°æ–¹ï¼Œåƒæ•¸èª¿æ•´ä¹Ÿæœƒä¸åŒã€‚

æ ¹æ“šæˆ‘å€‘ä¸Šé¢çš„è¨è«–ï¼Œæˆ‘å€‘è³‡æ–™é›†æœƒåˆ†æˆ __training set__ èˆ‡ __testing set__ ï¼Œæ‰€ä»¥æˆ‘å€‘èª¿æ•´è¶…åƒæ•¸çš„æ–¹å¼å°±æ˜¯ _(å‡è¨­æˆ‘å€‘åªè€ƒæ…®kçš„èª¿æ•´å¥½äº†)_

1. é¸æ“‡ä¸€å€‹kå€¼ï¼Œå»trainã€‚
2. é©—è­‰çœ‹çœ‹testing setçš„æº–ç¢ºç‡å¦‚ä½•ï¼Œå¦‚æœä¸æ»¿æ„ï¼Œåœ¨æ›´å‹•kå€¼
3. å›å»é‡æ–°train

è‡³æ­¤ï¼Œæ­å–œä½ ï¼Œç•¶ä½ æŠŠtrainå¥½çš„æ¨¡å‹æ”¾åˆ°å¯¦éš›æ‡‰ç”¨ç«¯æ™‚ï¼Œä½ æœƒç™¼ç¾ä½ æœ‰ __å¤§éº»~ç…©__ å›‰!
<br/><br/>
ç‚ºä»€éº¼?
<br/><br/>
å› ç‚ºéƒ¨ç½²åˆ°æ‡‰ç”¨ç«¯æ™‚ï¼Œä½ æ‰€æ¥æ”¶åˆ°çš„æ˜¯æ–°çš„è³‡æ–™é›†ï¼Œé€™äº›æ–°çš„è³‡æ–™é›†ä¸ä¸€å®šèƒ½å¤ å®Œå¥½çš„ç¬¦åˆä½ çš„modelï¼Œä¹Ÿå°±æ˜¯ä¸çŸ¥é“é€™å€‹æ¼”ç®—æ³•æ˜¯ä¸æ˜¯é©åˆæ–°çš„dataï¼Œæ¯”è¼ƒå¥½çš„ä½œæ³•æ˜¯

1. å°‡è³‡æ–™é›†åˆ‡æˆtraing, validation, testing
2. é¸æ“‡ä¸€å€‹kå€¼ï¼Œå»trainã€‚
3. é©—è­‰validation setçš„æº–ç¢ºç‡ï¼Œå¦‚æœä¸æ»¿æ„ï¼Œæ›´å‹•kå€¼å›å»é‡æ–°train
4. çµæœæ»¿æ„ä¹‹å¾Œï¼Œåœ¨testing seté€²è¡Œæ¸¬è©¦ã€‚

<br/><br/>

![img5_hyperparameter](/images/CS231_n/img_classification/img5_hyperparameter.PNG)

<br/><br/>

#### Cross-validation

ç•¶training dataå¤ªå°‘ï¼Œé‚£æˆ‘å€‘å°±æœƒä½¿ç”¨Cross-validationä¾†ä½œhyperparameter tuningã€‚

æ¯”å¦‚èªªæˆ‘å€‘æŠŠtraining set åˆ‡æˆ n ç­‰åˆ†ï¼ŒæŠŠå…¶ä¸­ä¸€å€‹foldç•¶ä½œæ˜¯validation setï¼Œç„¶å¾Œé€²è¡Œtrainingã€‚æ¥è‘—æŠŠvalidation setæ”¹ç‚ºä¸‹ä¸€å€‹foldï¼Œå†é€²è¡Œtrainingã€‚å¦‚æ­¤åè¦†ç¸½å…±ä½œnæ¬¡ï¼Œæœ€å¾Œå°‡é€™äº›performanceé€²è¡Œæˆæ•ˆæ¯”è¼ƒ
<br/><br/>

![img5_crossvalidation](/images/CS231_n/img_classification/img5_crossvalidation.PNG)
<br/><br/>

å¦‚æœå°å…¶ä»–çš„Cross-validationä¹Ÿæœ‰èˆˆè¶£ï¼Œå¯ä»¥çœ‹ä»¥ä¸‹çš„ä»‹ç´¹

[äº¤å‰é©—è­‰(Cross-validation, CV)](https://medium.com/@chih.sheng.huang821/%E4%BA%A4%E5%8F%89%E9%A9%97%E8%AD%89-cross-validation-cv-3b2c714b18db)

### ç¸½çµ

KNNçš„å„ªé»åœ¨æ–¼trainingçš„æ™‚é–“å¾ˆçŸ­ï¼Œå› ç‚ºåªæœ‰å°‡dataä½œåˆ‡å‰²èˆ‡å„²å­˜ã€‚ä½†æ˜¯åœ¨test timeå»å¾ˆæ¼«é•·ï¼Œå› ç‚ºå¿…é ˆå»æ¯”è¼ƒ **æ¯ä¸€å€‹** training imageã€‚ä½†æ˜¯æˆ‘å€‘æ‰€å¸Œæœ›çš„æ˜¯åœ¨testingæ™‚å¯ä»¥é¦¬ä¸Šå°±å¾—åˆ°yes or noçš„çµæœã€‚æ‰€ä»¥KNNå…¶å¯¦ä¸é©åˆç”¨åœ¨å½±åƒè¾¨è­˜ä¸Šã€‚

å¦‚æœä½ æƒ³è§€çœ‹åŸæ–‡çš„noteå¯ä»¥é»é¸
[cs231_n image classification notes](https://cs231n.github.io/classification/)

æƒ³çœ‹å¦‚ä½•å¯¦ä½œKNN [çœ‹æˆ‘å¦‚ä½•å¯¦ä½œknn (ongoing!!)]

é‚£éº¼ä¸‹ä¸€ç¯‡å°±æ˜¯ [Linear Classification]
